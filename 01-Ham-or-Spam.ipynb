{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ham or Spam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ The goal of this challenge is to classify emails as spams (1) or normal emails (0)\n",
    "\n",
    "üßπ First, you will apply cleaning techniques to these textual data\n",
    "\n",
    "üë©üèª‚Äçüî¨ Then, you will convert the cleaned texts into a numerical representation\n",
    "\n",
    "‚úâÔ∏è Eventually, you will apply the ***Multinomial Naive Bayes*** model to classify each email as either a spam or a regular email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The NTLK library (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/gonzalolara/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (3.7)\r\n",
      "Requirement already satisfied: click in /Users/gonzalolara/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from nltk) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /Users/gonzalolara/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from nltk) (1.1.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/gonzalolara/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from nltk) (2022.9.13)\r\n",
      "Requirement already satisfied: tqdm in /Users/gonzalolara/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from nltk) (4.64.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gonzalolara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gonzalolara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gonzalolara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/gonzalolara/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When importing nltk for the first time, we need to also download a few built-in libraries\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/10-Natural-Language-Processing/ham_spam_emails.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Cleaning the (text) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is made up of emails that are classified as ham [0] or spam[1]. You need to clean the dataset before training a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to remove the punctuation. Apply it to the `text` column and add the output to a new column in the dataframe called `clean_text` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       subject naturally irresistible your corporate ...\n",
       "1       subject the stock trading gunslinger  fanny is...\n",
       "2       subject unbelievable new homes made easy  im w...\n",
       "3       subject  color printing special  request addit...\n",
       "4       subject do not have money  get software cds fr...\n",
       "                              ...                        \n",
       "5723    subject re  research and development charges t...\n",
       "5724    subject re  receipts from visit  jim   thanks ...\n",
       "5725    subject re  enron case study update  wow  all ...\n",
       "5726    subject re  interest  david   please  call shi...\n",
       "5727    subject news  aurora    update  aurora version...\n",
       "Name: clean_text, Length: 5728, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def basic_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    \n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') \n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "df['clean_text'] = df['text'].apply(lambda sentence: basic_cleaning(sentence))\n",
    "\n",
    "df[\"clean_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to lowercase the text. Apply it to `clean_text` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Subject: naturally irresistible your corporate...\n",
       "1       Subject: the stock trading gunslinger  fanny i...\n",
       "2       Subject: unbelievable new homes made easy  im ...\n",
       "3       Subject: 4 color printing special  request add...\n",
       "4       Subject: do not have money , get software cds ...\n",
       "                              ...                        \n",
       "5723    Subject: re : research and development charges...\n",
       "5724    Subject: re : receipts from visit  jim ,  than...\n",
       "5725    Subject: re : enron case study update  wow ! a...\n",
       "5726    Subject: re : interest  david ,  please , call...\n",
       "5727    Subject: news : aurora 5 . 2 update  aurora ve...\n",
       "Name: text, Length: 5728, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "df[\"text\"][0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Remove Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to remove numbers from the text. Apply it to `clean_text` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Remove StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to remove stopwords from the text. Apply it to `clean_text`. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5) Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function to lemmatize the text. Make sure the output is a single string, not a list of words. Apply it to `clean_text`. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [subject, naturally, irresistible, your, corpo...\n",
       "1       [subject, the, stock, trading, gunslinger, fan...\n",
       "2       [subject, unbelievable, new, homes, made, easy...\n",
       "3       [subject, color, printing, special, request, a...\n",
       "4       [subject, do, not, have, money, get, software,...\n",
       "                              ...                        \n",
       "5723    [subject, re, research, and, development, char...\n",
       "5724    [subject, re, receipts, from, visit, jim, than...\n",
       "5725    [subject, re, enron, case, study, update, wow,...\n",
       "5726    [subject, re, interest, david, please, call, s...\n",
       "5727    [subject, news, aurora, update, aurora, versio...\n",
       "Name: clean_text, Length: 5728, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#para lematizar hay que tokenizar\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenization(sentence):\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    return tokenized_sentence\n",
    "\n",
    "df[\"clean_text\"].apply(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "verb_lemmatized = [                  \n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "    for word in df.clean_text\n",
    "]\n",
    "\n",
    "# 2 - Lemmatizing the nouns\n",
    "noun_lemmatized = [                 \n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "    for word in df.clean_text\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Bag-of-words Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Digitizing the textual data into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Vectorize the `clean_text` to a Bag-of-Words representation with a default CountVectorizer. Save as `X_bow`. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "test_values = df[\"clean_text\"].values\n",
    "X_bow = count_vectorizer.fit_transform(test_values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Multinomial Naive Bayes Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Cross-validate a MultinomialNB model with the bag-of-words data. Score the model's accuracy. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9890014251202206"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = MultinomialNB()\n",
    "cv_results = cross_validate(model, X_bow,y=df[\"spam\"], cv = 5, scoring = [\"accuracy\"])\n",
    "avg_accuracy = cv_results[\"test_accuracy\"].mean()\n",
    "avg_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations !\n",
    "\n",
    "üíæ Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "nothing to commit, working tree clean\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m\"excersise 100%\"\n",
    "!git push origin master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
